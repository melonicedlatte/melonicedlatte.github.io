---
layout : article
title: 엔트로피(Entropy)와 크로스 엔트로피(Cross-Entropy)의 개념
aside:
  toc: true
tags: MachineLearning
category : MachineLearning
author: melonicedlatte
published : True
key : 2019-12-20-204900
mathjax: true
---

## 1. Entropy
> 엔트로피는 `불확실성의 척도`입니다. 정보이론에서의 엔트로피는 `불확실성`을 나타내며, 엔트로피가 높다는 것은 정보가 많고, 확률이 낮다는 것을 의미합니다. 
> 
> $$ H(x)=−sum_{i=1}^{n}  P(x_i) log{P(x_i)} $$

이런 설명과 수식으로는 처음에는 와닿지가 않습니다. 제가 이해한 불확실성이라는 것은 **어떤 데이터가 나올지 예측하기 어려운 경우**라고 받아들이는 것이 더 직관적입니다. 예시를 통해 보는 것이 가장 좋습니다. 

### 1.1. 예시 _ 동전 던지기 & 주사위 던지기
- 동전을 던졌을 때, 앞/뒷면이 나올 확률을 모두 1/2라고 하겠습니다.
- 주사위를 던졌을 때,  각 6면이 나올 확률을 모두 1/6이라고 하겠습니다.

위의 두 상황에서 불확실성(어떤 데이터가 나올지 예측하기 어려운 것)은 주사위가 더 크다고 직관적으로 다가옵니다. 이를 수식적으로 계산하면 각각 아래와 같습니다.

- $$ H(x)= -( \frac{1}{2} \left( \frac{1}{2} \right) + \frac{1}{2} \left( \frac{1}{2} \right)  ) $$

## 2. Cross-Entropy

---
**출처**
- https://3months.tistory.com/436
- https://m.blog.naver.com/PostView.nhn?blogId=roboholic84&logNo=221629115916&proxyReferer=https%3A%2F%2Flm.facebook.com%2F
