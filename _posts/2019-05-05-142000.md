---
layout : article
title:  강화학습 MDP와 벨만 방정식 기본 개념
aside:
  toc: true
tags: MachineLearning
category : MachineLearning
author: melonicedlatte
published : True
# cover : /assets/images/logo/chihuahua.jpg
key : 2019-05-05-142000
mathjax: true 
---

## MDP(Markov Decision Process)

강화학습은 **순차적으로 진행되는 일**에서 행동을 결정해야 합니다. 이를 **수학적으로 결정한 것이 MDP** 입니다. 모든 것을 수치화하여 수학적으로 결정해야만 컴퓨터가 판단할 수 있습니다.

- 구성 요소 : 상태, 행동, 보상함수, 상태 변환 확률, 감가율, 정책

문제를 잘 정의해야 강화학습 컴퓨터인 agent 가 제대로 학습할 수 있습니다.

### 상태

상태 S는 에이전트가 **관찰 가능한 상태의 집합**입니다.  
ex) 체스에서 말들의 좌표, 축구 게임에서 선수 캐릭터들이 공을 가지고 있는지 아닌지의 여부 등

$$S = \{ (x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n) \}$$

**상태의 집합**{:.caption}

시간 t 일때의 상태를 $ S_t $ 라고 합니다. 시간 t 일 때, 체스 말이 (3, 5)에 있다면 아래와 같이 표현할 수 있습니다.

$$S_t = (3, 5) $$

**$s_t$ 에서의 좌표를 표시하는 수식**{:.caption}

### 행동



### 보상함수

### 상태 변환 확률

### 감가율

### 정책

## 가치함수

### 큐함수

## 벨만 방정식

### 벨만 기대 방정식

### 벨만 최적 방정식