---
layout : article
title:  강화학습 MDP의 구성 요소들에 대한 개념
aside:
  toc: true
tags: MachineLearning
category : MachineLearning
author: melonicedlatte
published : True
# cover : /assets/images/logo/chihuahua.jpg
key : 2019-05-05-142000
mathjax: true 
---

## MDP(Markov Decision Process)

강화학습은 **순차적으로 진행되는 일**에서 행동을 결정해야 합니다. 이를 **수학적으로 결정한 것이 MDP** 입니다. 모든 것을 수치화하여 수학적으로 결정해야만 컴퓨터가 판단할 수 있습니다.

- 구성 요소 : 상태, 행동, 보상함수, 상태 변환 확률, 감가율, 정책

문제를 잘 정의해야 강화학습 컴퓨터인 agent 가 제대로 학습할 수 있습니다.

### 상태

상태 S는 에이전트가 **관찰 가능한 상태의 집합**입니다.  
ex) 체스에서 말들의 좌표, 축구 게임에서 선수 캐릭터들이 공을 가지고 있는지 아닌지의 여부 등

$$S = \{ (x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n) \}$$

**[수식]** 상태의 집합{:.mathjax_caption}

시간 t 일때의 상태를 $S_t$ 라고 합니다. 시간 $t$ 일 때, 체스 말이 $(3, 5)$에 있다면 아래와 같이 표현할 수 있습니다.

$$S_t = (3, 5)$$

**[수식]** $S_t$ 에서의 (3,5) 상태 표시{:.mathjax_caption}

$$S_t = s $$

**[수식]** $S_t$ 에서의 상태 $s${:.mathjax_caption}

상태는 **어떤 집합 안에서 뽑을 때마다 달라질 수 있는 확률변수** 입니다.

### 행동

에이전트가 상태 $S_t$ 에서 할 수 있는 가능한 행동의 집합은 $A$ 입니다. $A_t$는 시간 $t$에 집합 A에서 선택한 행동입니다.

$$A_t = a$$

**[수식]** $A_t$ 에서의 행동 $a$ {:.mathjax_caption}

만약, 달리거나 뛰는 행동 중에서 고를 수 있다면 집합 $A$는 아래와 같이 표현할 수 있습니다.

$$A = \{ run, walk \}$$

**[수식]** $A$ 에서 가능한 행동의 집합 $a${:.mathjax_caption}

### 보상함수

보상은 에이전트가 학습할 수 있는 **유일한 정보**로서 환경이 에이전트에게 주는 정보입니다. 시간 $t$에서 상태가 $S_t = s$이고, 행동이 $A_t = a$일 때, 에이전트가 받을 보상은 아래와 같습니다.

$$R^a_s = E[R_{t+1} | S_t = s, A_t = a]$$

**[수식]** 보상함수의 정의{:.mathjax_caption}

보상함수는 시간 $t$일 때, 받을 보상에 대한 기댓값 $E$입니다. 기댓값은 실제 값이 아니라 **예측 값**입니다. 환경에 따라서 **같은 상태에서 같은 행동을 해도 다른 보상을 받을 수 있기 때문에 기댓값으로 표현**합니다.

에이전트의 행동 시간은 $t$이지만, 보상을 받는 시점은 $t + 1$ 입니다. 행동을 하고 나서야 비로소 보상을 알 수 있기 때문입니다.

### 상태 변환 확률

에이전트가 어떤 상태에서 어떤 행동을 취할 때, 상태 $s$에서 $s'$로 다음 시간에 도달 할 수도 있지만 **도달하지 못하는 경우도 있습니다.** 항상 행동 $a$를 취했다고 해서 원하는 상태 $s'$로 이동하는 것이 아닙니다. 예를 들어, 비가 와서 축구 게임 속의 플레이어가 공을 제대로 차지 못할 수도 있습니다. 이를 수치적으로 표현한 것이 상태 변환 확률입니다.

$$P^a_{ss'} = P[S_{t+1} = s' | S_t = s, A_t = a]$$

**[수식]** 상태 변환 확률의 정의{:.mathjax_caption}

풀어서 위의 식을 설명하면, 현재 시간 $t$ 에서 상태 $s$ 이고 행동 $a$를 했을 때, 시간 $t+1$에서 상태가 $s'$가 될 확률입니다.

### 감가율

에이전트가 **가까운 보상일수록 더 큰 가치를 주고, 먼 보상일수록 더 작은 가치를 주기** 위해서 도입한 개념입니다. 에이전트가 현재에 판단을 내리기 때문에, 현재에 가까운 보상일수록 더 큰 가치를 가지기 때문입니다.

쉽게 생각하기 위하여 예시를 하나 들겠습니다. 돈의 가치가 시간이 지날수록 떨어지기 때문에, 동일한 1만원이여도 지금 받는게 50년 뒤에 받는 것보다 가치가 높습니다. 강화학습에서도 에이전트가 현재에 가까우누 보상일수록 더 큰 가치라고 생각하기 때문에, 미래의 가치에 감가율 $\gamma \in [0,1]$를 곱해서 가치를 판단합니다.

$$\gamma^{k-1}R_{t+k}$$

**[수식]** 감가율을 고려한 시간 k 만큼 뒤에서의 미래 보상의 현재가치{:.mathjax_caption}

### 정책

정책은 모든 상태에서 **에이전트가 어떻게 행동을 할 것인가**를 정해주는 것입니다.

$$\pi(a|s) = P[A_t = a | S_t = s]$$

**[수식]** 정책의 정의{:.mathjax_caption}

위 식은 시간 $t$에 상태 $s$ 일 때, 가능한 행동 중에서 A_t = a 를 할 확률을 나타냅니다. 어떤 행동을 할 확률을 의미합니다. 강화학습을 할 때는 가장 좋은 정책인 **"최적 정책"**을 얻기 위해서, 현재의 정책보다 더 좋은 정책을 학습해 나갑니다.

![image](/assets/images/20190505/Reinforcement_learning.jpg){:.rounded.img_center.width_40}
**에이전트와 환경의 관계**{:.caption}

위의 관계에서 지속적으로 보상을 받으면서, 에이전트가 **앞으로 받을 것이라고 예상한 보상(가치함수)** 가 틀렸다는 것을 알게 됩니다. 이를 지속적으로 갱신해나가며 최적 정책을 찾는 것입니다.