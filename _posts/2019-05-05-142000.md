---
layout : article
title:  강화학습 MDP와 벨만 방정식 기본 개념
aside:
  toc: true
tags: MachineLearning
category : MachineLearning
author: melonicedlatte
published : True
# cover : /assets/images/logo/chihuahua.jpg
key : 2019-05-05-142000
mathjax: true 
---

## MDP(Markov Decision Process)

강화학습은 **순차적으로 진행되는 일**에서 행동을 결정해야 합니다. 이를 **수학적으로 결정한 것이 MDP** 입니다. 모든 것을 수치화하여 수학적으로 결정해야만 컴퓨터가 판단할 수 있습니다.

- 구성 요소 : 상태, 행동, 보상함수, 상태 변환 확률, 감가율, 정책

문제를 잘 정의해야 강화학습 컴퓨터인 agent 가 제대로 학습할 수 있습니다.

### 상태

상태 S는 에이전트가 **관찰 가능한 상태의 집합**입니다.  
ex) 체스에서 말들의 좌표, 축구 게임에서 선수 캐릭터들이 공을 가지고 있는지 아닌지의 여부 등

$$S = \{ (x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n) \}$$

**상태의 집합**{:.mathjax_caption}

시간 t 일때의 상태를 $S_t$ 라고 합니다. 시간 $t$ 일 때, 체스 말이 $(3, 5)$에 있다면 아래와 같이 표현할 수 있습니다.

$$S_t = (3, 5)$$

**$S_t$ 에서의 (3,5) 상태 표시**{:.mathjax_caption}

$$S_t = s $$

**$S_t$ 에서의 상태 $s$**{:.mathjax_caption}

상태는 **어떤 집합 안에서 뽑을 때마다 달라질 수 있는 확률변수** 입니다.

### 행동

에이전트가 상태 $S_t$ 에서 할 수 있는 가능한 행동의 집합은 $A$ 입니다. $A_t$는 시간 $t$에 집합 A에서 선택한 행동입니다.

$$A_t = a$$

**$A_t$ 에서의 행동 $a$**{:.mathjax_caption}

만약, 달리거나 뛰는 행동 중에서 고를 수 있다면 집합 $A$는 아래와 같이 표현할 수 있습니다.

$$A = \{ run, walk \}$$

**$A$ 에서 가능한 행동의 집합 $a$**{:.mathjax_caption}

### 보상함수

보상은 에이전트가 학습할 수 있는 **유일한 정보**로서 환경이 에이전트에게 주는 정보입니다. 시간 $t$에서 상태가 $S_t = s$이고, 행동이 $A_t = a$일 때, 에이전트가 받을 보상은 아래와 같습니다.

$$R^a_s = E[R_{t+1} \enspace|\enspace S_t = s, A_t = a]$$

**보상함수의 정의**{:.mathjax_caption}

보상함수는 시간 $t$일 때, 받을 보상에 대한 기댓값 $E$입니다. 기댓값은 실제 값이 아니라 **예측 값**입니다. 환경에 따라서 **같은 상태에서 같은 행동을 해도 다른 보상을 받을 수 있기 때문에 기댓값으로 표현**합니다.

에이전트의 행동 시간은 $t$이지만, 보상을 받는 시점은 $t + 1$ 입니다. 행동을 하고 나서야 비로소 보상을 알 수 있기 때문입니다.

### 상태 변환 확률

에이전트가 어떤 상태에서 어떤 행동을 취할 때, 상태 $s$에서 $s'$로 다음 시간에 도달 할 수도 있지만 **도달하지 못하는 경우도 있습니다.** 항상 행동 $a$를 취했다고 해서 원하는 상태 $s'$로 이동하는 것이 아닙니다. 예를 들어, 비가 와서 축구 게임 속의 플레이어가 공을 제대로 차지 못할 수도 있습니다. 이를 수치적으로 표현한 것이 상태 변환 확률입니다.

$$P^a_{ss'} = P[S_{t+1} = s' \enspace | \enspace S_t = s, A_t = a]$$

**상태 변환 확률의 정의**{:.mathjax_caption}

풀어서 위의 식을 설명하면, 현재 시간 $t$ 에서 상태 $s$ 이고 행동 $a$를 했을 때, 시간 $t+1$에서 상태가 $s'$가 될 확률입니다.

### 감가율

에이전트가 **가까운 보상일수록 더 큰 가치를 주고, 먼 보상일수록 더 작은 가치를 주기** 위해서 도입한 개념입니다. 에이전트가 현재에 판단을 내리기 때문에, 현재에 가까운 보상일수록 더 큰 가치를 가지기 때문입니다.

쉽게 생각하기 위하여 예시를 하나 들겠습니다. 돈의 가치가 시간이 지날수록 떨어지기 때문에, 동일한 1만원이여도 지금 받는게 50년 뒤에 받는 것보다 가치가 높습니다. 강화학습에서도 에이전트가 현재에 가까우누 보상일수록 더 큰 가치라고 생각하기 때문에, 미래의 가치에 감가율 $\gamma \in [0,1]$를 곱해서 가치를 판단합니다.

$$\gamma^{k-1}R_{t+k}$$

**감가율을 고려한 시간 k 만큼 뒤에서의 미래 보상의 현재가치**{:.mathjax_caption}

### 정책

정책은 모든 상태에서 **에이전트가 어떻게 행동을 할 것인가**를 정해주는 것입니다.

$$\pi(a|s) = P[A_t = a | S_t = s]$$

**정책의 정의**{:.mathjax_caption}

위 식은 시간 $t$에 상태 $s$ 일 때, 가능한 행동 중에서 A_t = a 를 할 확률을 나타냅니다. 어떤 행동을 할 확률을 의미합니다. 강화학습을 할 때는 가장 좋은 정책인 **"최적 정책"**을 얻기 위해서, 현재의 정책보다 더 좋은 정책을 학습해 나갑니다.

![image](/assets/images/20190505/Reinforcement_learning.jpg){:.rounded.img_center.width_40}
**에이전트와 환경의 관계**{:.caption}

위의 관계에서 지속적으로 보상을 받으면서, 에이전트가 **앞으로 받을 것이라고 예상한 보상(가치함수)** 가 틀렸다는 것을 알게 됩니다. 이를 지속적으로 갱신해나가며 최적 정책을 찾는 것입니다.

<br>

## 가치함수

에이전트가 최적 정책을 찾기 위해서는 **앞으로 받을 보상들을 고려**해야 합니다. 아직 받지 않은 보상들을 고려하기 위하여 가치함수가 필요합니다.

$$ R_{t+1} + R_{t+2} + R_{t+3} + \cdots$$

**보상들의 합**{:.mathjax_caption}

위와 같이 **감가하지 않고 보상들을 더하면 아래와 같은 문제**가 생길 수 있습니다.

1. 지금 받은 보상이나 미래에 받는 보상이 같아 구분할 수 없습니다.
2. 큰 보상을 한 번 받은 것과 작은 보상을 여러번 받은 것을 구분할 수 없습니다.
3. 시간이 무한대이면 10을 계속 더한 것과 100을 계속 더한 것을 구분할 수 없습니다.

감가율을 고려하면 위의 식을 아래와 같이 나타낼 수 있고, 이를 반환값 $G_t$ 라고 합니다.

$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3} + \cdots$$

**보상들의 합**{:.mathjax_caption}

에이전트는 위와 같은 반환값 $G_t$를 알기 위해서 에피소드가 모두 끝날 때 까지 기다려야 합니다. 하지만, 때로는 정확한 값을 얻기 위해 끝까지 기다리는 것보다 **정확하지 않더라도 현재의 정보를 가지고 행동**하는 것이 나을 때가 있습니다.

어떤 상태에 있으면 앞으로 얼마의 보상을 받을 것인지에 대한 기댓값을 고려해볼 수 있는데 이를 **가치함수**라고 합니다.

$$ \text{v}(s) = E[G_t|S_t = s] $$

**가치함수의 정의**{:.mathjax_caption}

반환값 $G_t$는 보상이 모두 확률변수이기 때문에 확률변수이지만, 가치함수는 E로 평균을 낸 기대값이기 때문에 **소문자**로 표현합니다. 가치함수는 아래와 같이 반환값으로 식을 표현할 수 있습니다.

$$ \text{v}(s) = E[G_t|S_t = s] = E[R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3}+\cdots|S_t = s]  $$

$$ \text{v}(s) = E[R_{t+1} + \gamma(R_{t+2} + \gamma^{1}R_{t+3}+\cdots)|S_t = s]  $$

$$ \text{v}(s) = E[R_{t+1} + \gamma G_{t+1}|S_t = s]  $$

**가치함수에서 반환값 $G_t$와 $G_{t+1}$로 식 표현**{:.mathjax_caption}

반환값을 다시 가치함수로 변경하면 아래와 같이 나타낼 수 있습니다.

$$ \text{v}(s) = E[R_{t+1} + \gamma \text{v}(S_{t+1})|S_t = s]  $$

**가치함수로 가치함수를 표현**{:.mathjax_caption}

위의 식은 정책을 고려하지 않았습니다. **어떤 상태에서 어떤 행동을 해야 할 지에 대한 정보** 인 정책을 고려하여 식을 다시 표현하면 아래와 같습니다.

$$ \text{v}_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma \text{v}_{\pi}(S_{t+1})|S_t = s]  $$

**정책을 고려한 가치함수의 표현**{:.mathjax_caption}

### 큐함수

## 벨만 방정식

### 벨만 기대 방정식

### 벨만 최적 방정식