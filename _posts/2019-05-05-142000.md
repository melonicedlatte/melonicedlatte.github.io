---
layout : article
title:  강화학습 MDP와 벨만 방정식 기본 개념
aside:
  toc: true
tags: MachineLearning
category : MachineLearning
author: melonicedlatte
published : True
# cover : /assets/images/logo/chihuahua.jpg
key : 2019-05-05-142000
mathjax: true 
---

## MDP(Markov Decision Process)

강화학습은 **순차적으로 진행되는 일**에서 행동을 결정해야 합니다. 이를 **수학적으로 결정한 것이 MDP** 입니다. 모든 것을 수치화하여 수학적으로 결정해야만 컴퓨터가 판단할 수 있습니다.

- 구성 요소 : 상태, 행동, 보상함수, 상태 변환 확률, 감가율, 정책

문제를 잘 정의해야 강화학습 컴퓨터인 agent 가 제대로 학습할 수 있습니다.

### 상태

상태 S는 에이전트가 **관찰 가능한 상태의 집합**입니다.  
ex) 체스에서 말들의 좌표, 축구 게임에서 선수 캐릭터들이 공을 가지고 있는지 아닌지의 여부 등

$$S = \{ (x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n) \}$$

**상태의 집합**{:.caption}

시간 t 일때의 상태를 $S_t$ 라고 합니다. 시간 $t$ 일 때, 체스 말이 $(3, 5)$에 있다면 아래와 같이 표현할 수 있습니다.

$$S_t = (3, 5)$$

**$S_t$ 에서의 (3,5) 상태 표시**{:.caption}

$$S_t = s $$

**$S_t$ 에서의 상태 $s$**{:.caption}

상태는 **어떤 집합 안에서 뽑을 때마다 달라질 수 있는 확률변수** 입니다.

### 행동

에이전트가 상태 $S_t$ 에서 할 수 있는 가능한 행동의 집합은 $A$ 입니다. $A_t$는 시간 $t$에 집합 A에서 선택한 행동입니다.

$$A_t = a$$

**$A_t$ 에서의 행동 $a$**{:.caption}

만약, 달리거나 뛰는 행동 중에서 고를 수 있다면 집합 $A$는 아래와 같이 표현할 수 있습니다.

$$A = \{ run, walk \}$$

**$A$ 에서 가능한 행동의 집합 $a$**{:.caption}

### 보상함수

보상은 에이전트가 학습할 수 있는 **유일한 정보**로서 환경이 에이전트에게 주는 정보입니다. 시간 $t$에서 상태가 $S_t = s$이고, 행동이 $A_t = a$일 때, 에이전트가 받을 보상은 아래와 같습니다.

$$R^a_s = E[R_{t+1} \enspace|\enspace S_t = s, A_t = a]$$

**보상함수의 정의**{:.caption}

보상함수는 시간 $t$일 때, 받을 보상에 대한 기댓값 $E$입니다. 기댓값은 실제 값이 아니라 **예측 값**입니다. 환경에 따라서 **같은 상태에서 같은 행동을 해도 다른 보상을 받을 수 있기 때문에 기댓값으로 표현**합니다.

에이전트의 행동 시간은 $t$이지만, 보상을 받는 시점은 $t + 1$ 입니다. 행동을 하고 나서야 비로소 보상을 알 수 있기 때문입니다.

### 상태 변환 확률

### 감가율

### 정책

## 가치함수

### 큐함수

## 벨만 방정식

### 벨만 기대 방정식

### 벨만 최적 방정식