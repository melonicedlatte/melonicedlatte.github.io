---
layout : article
title:  가치 함수와 벨만 방정식의 개념
aside:
  toc: true
tags: MachineLearning
category : MachineLearning
author: melonicedlatte
published : True
# cover : /assets/images/logo/chihuahua.jpg
key : 2019-05-05-205300
mathjax: true 
---

## 가치함수

에이전트가 최적 정책을 찾기 위해서는 **앞으로 받을 보상들을 고려**해야 합니다. 아직 받지 않은 보상들을 고려하기 위하여 가치함수가 필요합니다.

$$ R_{t+1} + R_{t+2} + R_{t+3} + \cdots$$

**[수식]** 보상들의 합{:.mathjax_caption}

위와 같이 **감가하지 않고 보상들을 더하면 아래와 같은 문제**가 생길 수 있습니다.

1. 지금 받은 보상이나 미래에 받는 보상이 같아 구분할 수 없습니다.
2. 큰 보상을 한 번 받은 것과 작은 보상을 여러번 받은 것을 구분할 수 없습니다.
3. 시간이 무한대이면 10을 계속 더한 것과 100을 계속 더한 것을 구분할 수 없습니다.

감가율을 고려하면 위의 식을 아래와 같이 나타낼 수 있고, 이를 반환값 $G_t$ 라고 합니다.

$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3} + \cdots$$

**[수식]** 보상들의 합{:.mathjax_caption}

에이전트는 위와 같은 반환값 $G_t$를 알기 위해서 에피소드가 모두 끝날 때 까지 기다려야 합니다. 하지만, 때로는 정확한 값을 얻기 위해 끝까지 기다리는 것보다 **정확하지 않더라도 현재의 정보를 가지고 행동**하는 것이 나을 때가 있습니다.

어떤 상태에 있으면 앞으로 얼마의 보상을 받을 것인지에 대한 기댓값을 고려해볼 수 있는데 이를 **가치함수**라고 합니다.

$$ \text{v}(s) = E[G_t|S_t = s] $$

**[수식]** 가치함수의 정의{:.mathjax_caption}

반환값 $G_t$는 보상이 모두 확률변수이기 때문에 확률변수이지만, 가치함수는 E로 평균을 낸 기대값이기 때문에 **소문자**로 표현합니다. 가치함수는 아래와 같이 반환값으로 식을 표현할 수 있습니다.

$$ \text{v}(s) = E[G_t|S_t = s] = E[R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3}+\cdots|S_t = s]  $$

$$ \text{v}(s) = E[R_{t+1} + \gamma(R_{t+2} + \gamma^{1}R_{t+3}+\cdots)|S_t = s]  $$

$$ \text{v}(s) = E[R_{t+1} + \gamma G_{t+1}|S_t = s]  $$

**[수식]** 가치함수에서 반환값 $G_t$와 $G_{t+1}$로 식 표현{:.mathjax_caption}

반환값을 다시 가치함수로 변경하면 아래와 같이 나타낼 수 있습니다.

$$ \text{v}(s) = E[R_{t+1} + \gamma \text{v}(S_{t+1})|S_t = s]  $$

**[수식]** 가치함수로 가치함수를 표현{:.mathjax_caption}

위의 식은 정책을 고려하지 않았습니다. **어떤 상태에서 어떤 행동을 해야 할 지에 대한 정보** 인 정책을 고려하여 식을 다시 표현하면 아래와 같습니다.

$$ \text{v}_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma \text{v}_{\pi}(S_{t+1})|S_t = s]  $$

**[수식]** 정책을 고려한 가치함수의 표현{:.mathjax_caption}

### 큐함수

## 벨만 방정식

### 벨만 기대 방정식

### 벨만 최적 방정식