---
layout : article
title: 그리드월드와 다이나믹 프로그래밍
aside:
  toc: true
tags: MachineLearning
category : MachineLearning
author: melonicedlatte
published : True
# cover : /assets/images/logo/chihuahua.jpg
key : 2019-05-06-152500
mathjax: true 
---

*파이썬과 케라스로 배우는 강화학습* 책을 보고 간략하게 배운 개념을 요약했습니다. 실제 책에는 많은 사례와 그림들이 나와있으니, 실제로 사서 보시면 입문자분께 많은 도움이 될 꺼라고 생각합니다.

<br>

## 다이나믹 프로그래밍과 그리드 월드

### 순차적 행동 결정 문제

강화학습은 순차적으로 행동을 결정하는 문제를 푸는 방법입니다. 순차적 문제를 MDP로 설정해주고, 가치함수를 벨만 방정식을 통해서 반복해서 계산한 후에, 최적 가치함수와 최적 정책을 찾으면 됩니다. 

### 다이나믹 프로그래밍

큰 문제를 작게 쪼개서 푸는 것이 다이나믹 프로그래밍입니다. 

$$ 
\text{v}_{0}(\text{s}) + \text{v}_{1}(\text{s}) + \cdots + \text{v}_{\pi}(\text{s})
$$

**수식.** 가치함수를 구하는 과정을 작게 쪼개서 계산
{:.mathjax_caption}

다이나믹 프로그래밍으로 벨만 방정식을 풀어 문제를 해결합니다. 다이나믹 프로그래밍에는 2가지가 있습니다.

- **정책 이터레이션 (Policy Iteration)** : 벨만 기대 방정식으로 문제 해결
- **가치 이터레이션 (Value Iteration)** : 벨만 최적 방정식으롤 문제 해결

<br>

## 다이나믹 프로그래밍 1 : 정책 이터레이션

### 강화학습 알고리즘의 흐름

정책 이터레이션과 가치 이터레이션은 후에 살사(SARSA)로 발전하고, 살사는 오프폴리시(off policy) 방법으로 변형되어 큐러닝(Q Learning)으로 이어집니다.

### 정책 이터레이션

처음에는 가장 높은 보상을 얻게 하는 정책을 모르니 특정 정책을 시작으로 하여 계속 갱신합니다. 정책이 얼마나 좋은지 평가하여 정책을 지속적으로 발전시켜 나갑니다. 

![image](/assets/images/20190506/policy_cycle.jpg){:.rounded.img_center.width_40}
**정책 평가와 정책 발전으로 정책을 계속 갱신한다.**{:.caption}

### 정책 평가

정책 평가에서는 가치함수가 근거가 됩니다. 정책 이터레이션에서는 다이나믹 프로그래밍을 수행하기 위하여 벨만 기대 방정식을 사용합니다. 이는 **주변 상태의 가치함수와 한 타입 스텝의 보상만 고려하여 다음 가치 함수를 구한다**는 뜻입니다. 주변 가치함수가 실제 값이 아니더라도 반복하다보면 참 값으로 수렴합니다. 

$$ 
\text{v}_{\text{k}+1}(s) = \sum_{a \in A} \pi(a|s)(R_{t+1} + \gamma  \text{v}_{\text{k}}(s')) 
$$

**수식.** $\text{k}$번째 가치함수를 통해 $\text{k+1}$ 구하기
{:.mathjax_caption}

위의 수식을 무한하게 반복하면 참값에 도달할 수 있습니다. 

### 정책 발전
$$ 
\text{q}_{\pi}(s,a) = R^a_s + \gamma \text{v}_{\pi}(s') 
$$

**수식.** 계산 가능한 형태의 큐함수
{:.mathjax_caption}

정책을 발전 시키려면 여러가지 방법이 있지만 탐욕 정책 발전만 알아 보겠습니다. 위의 계산 가능한 큐함수를 이용하여 가장 값이 큰 큐함수를 선택하여 정책에 반영하는 것이 탐용 정책 발전입니다. 

### 예제를 이용한 테스트

![image](/assets/images/20190506/policy_iteration_simulation.jpg){:.rounded.img_center.width_40}
**실제 evaluation 으로 평가하고 improvement를 통해서 향상시키는 동작을 반복하면 최적경로가 정해진다.**{:.caption}

<br>

## 다이나믹 프로그래밍 2 : 가치 이터레이션

### 명시적인 정책과 내재적인 정책

### 벨만 최적 방정식과 가치 이터레이션

<br>

## 다이나믹 프로그래밍의 한계와 강화학습

### 다이나믹 프로그래밍의 한계

### 모델 없이 학습하는 강화학습

