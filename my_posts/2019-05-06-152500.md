---
layout : article
title: 그리드월드와 다이나믹 프로그래밍
aside:
  toc: true
tags: MachineLearning
category : MachineLearning
author: melonicedlatte
published : True
key : 2019-05-06-152500
mathjax: true 
---

*파이썬과 케라스로 배우는 강화학습* 책을 보고 간략하게 배운 개념을 요약했습니다. 실제 책에는 많은 사례와 그림들이 나와있으니, 실제로 사서 보시면 입문자분께 많은 도움이 될 꺼라고 생각합니다.

<br>

## 다이나믹 프로그래밍과 그리드 월드

### 순차적 행동 결정 문제

강화학습은 순차적으로 행동을 결정하는 문제를 푸는 방법입니다. 순차적 문제를 MDP로 설정해주고, 가치함수를 벨만 방정식을 통해서 반복해서 계산한 후에, 최적 가치함수와 최적 정책을 찾으면 됩니다. 

### 다이나믹 프로그래밍

큰 문제를 작게 쪼개서 푸는 것이 다이나믹 프로그래밍입니다. 

$$ 
\text{v}_{0}(\text{s}) + \text{v}_{1}(\text{s}) + \cdots + \text{v}_{\pi}(\text{s})
$$

**수식.** 가치함수를 구하는 과정을 작게 쪼개서 계산
{:.mathjax_caption}

다이나믹 프로그래밍으로 벨만 방정식을 풀어 문제를 해결합니다. 다이나믹 프로그래밍에는 2가지가 있습니다.

- **정책 이터레이션 (Policy Iteration)** : 벨만 기대 방정식으로 문제 해결
- **가치 이터레이션 (Value Iteration)** : 벨만 최적 방정식으롤 문제 해결

<br>

## 다이나믹 프로그래밍 1 : 정책 이터레이션

### 강화학습 알고리즘의 흐름

정책 이터레이션과 가치 이터레이션은 후에 살사(SARSA)로 발전하고, 살사는 오프폴리시(off policy) 방법으로 변형되어 큐러닝(Q Learning)으로 이어집니다.

### 정책 이터레이션

처음에는 가장 높은 보상을 얻게 하는 정책을 모르니 특정 정책을 시작으로 하여 계속 갱신합니다. 정책이 얼마나 좋은지 평가하여 정책을 지속적으로 발전시켜 나갑니다. 

![image](/assets/images/20190506/policy_cycle.jpg){:.rounded.img_center.width_40}
**정책 평가와 정책 발전으로 정책을 계속 갱신한다.**{:.caption}

### 정책 평가

정책 평가에서는 가치함수가 근거가 됩니다. 정책 이터레이션에서는 다이나믹 프로그래밍을 수행하기 위하여 벨만 기대 방정식을 사용합니다. 이는 **주변 상태의 가치함수와 한 타입 스텝의 보상만 고려하여 다음 가치 함수를 구한다**는 뜻입니다. 주변 가치함수가 실제 값이 아니더라도 반복하다보면 참 값으로 수렴합니다. 

$$ 
\text{v}_{\text{k}+1}(s) = \sum_{a \in A} \pi(a|s)(R_{t+1} + \gamma  \text{v}_{\text{k}}(s')) 
$$

**수식.** $\text{k}$번째 가치함수를 통해 $\text{k+1}$ 구하기
{:.mathjax_caption}

위의 수식을 무한하게 반복하면 참값에 도달할 수 있습니다. 

### 정책 발전

$$ 
\text{q}_{\pi}(s,a) = R^a_s + \gamma \text{v}_{\pi}(s') 
$$

**수식.** 계산 가능한 형태의 큐함수
{:.mathjax_caption}

정책을 발전 시키려면 여러가지 방법이 있지만 탐욕 정책 발전만 알아 보겠습니다. 위의 계산 가능한 큐함수를 이용하여 가장 값이 큰 큐함수를 선택하여 정책에 반영하는 것이 탐용 정책 발전입니다. 

### 예제를 이용한 테스트

![image](/assets/images/20190506/policy_iteration_simulation.jpg){:.rounded.img_center.width_25}
**실제 evaluation 으로 평가하고 improvement를 통해서 향상시키는 동작을 반복하면 최적경로가 정해진다.**{:.caption}

<br>

## 다이나믹 프로그래밍 2 : 가치 이터레이션

### 명시적인 정책과 내재적인 정책

`정책 이터레이션`은 **명시적인 정책이 있으며 정책 평가로 가치함수를 사용**합니다. 이터레이션을 반복할수록 정책이 최적에 도달해갑니다. 정책과 가치함수가 명확하게 분리되어있습니다. 

`가치 이터레이션`에서는 **현재의 가치함수를 최적이라고 가정하**고 결정적인 형태를 적용합니다. 이상해보이지만 반복적으로 계산을 수행하면 결국 최적에 도달합니다. **가치함수 안에 정책이 내재**되어 있으므로 가치함수를 업데이트하면 자동으로 정책도 발전합니다.

### 벨만 최적 방정식과 가치 이터레이션

$$ \text{v}_{*}(s) = \max_{a}E[R_{t+1} + \gamma \text{v}_{*}(S_{t+1})|S_t = s, A_t = a] $$

**수식.** 벨만 최적 방정식
{:.mathjax_caption}

**벨만 최적 방정식**은 벨만 기대 방정식과 달리 max를 취해서 **정책을 고려할 필요가 없습니다.** 가능한 $ R_{t+1} + \gamma \text{v}_{\text{k}}(S_{t+1}) $의 값들 중에서 최고의 값들로 업데이트 합니다. 

벨만 최적 방정식을 계산 간으한 형태로 변환하면 아래의 수식과 같게 나옵니다. 벨만 기대 방정식의 정책값을 계산하는 과정이 없고 max가 생겼습니다. 

$$ \text{v}_{\text{k+1}}(s) = \max_{a \in A}(R^a_s + \gamma \text{v}_{\text{k}}(s')) $$

**수식.** 계산 가능한 벨만 최적 방정식
{:.mathjax_caption}

<br>

## 다이나믹 프로그래밍의 한계와 강화학습

### 다이나믹 프로그래밍의 한계

다이나믹 프로그래밍은 계산을 빠르게 하는 것이고 **학습이 아닙니다.** 다이나믹 프로그래밍의 한계는 아래와 같습니다. 

- 계산 복잡도 : 다이나믹 프로그래밍의 계산 복잡도는 $O(\text{상태크기}^3)$
- 차원의 저주 : 차원이 늘어나면 계산량이 급증
- 환경에 대한 완벽한 정보 필요

### 모델 없이 학습하는 강화학습

MDP에서 환경 모델은 상태 변환 확률과 보상입니다. 

$$
\text{환경의 모델} = P^a_{ss'}, R^a_s
$$

**수식.** MDP에서 환경의 모델은 상태 변환 확률과 보상
{:.mathjax_caption}

시스템에 입력이 들어왔을 때 시스템이 어떤 출력을 내는지에 대한 방정식을 만드는 것은 `모델링`입니다. 모델은 정확하면 정확할수록 정확하지만, 자연 환경에서 존재하는 모든 조건들을 고려할 수 없습니다. 

`강화학습`에서는 **모든 상황에서 동일하게 작동된다는 보장은 없지만, 많은 복잡한 문제에서 모델이 필요없다는 장점이 있습니다.** 
