---
layout : article
title: 그리드월드와 큐러닝
aside:
  toc: true
tags: MachineLearning
category : MachineLearning
author: melonicedlatte
published : False
key : 2019-05-12-145400
mathjax: true 
---

*파이썬과 케라스로 배우는 강화학습* 책을 보고 간략하게 배운 개념을 요약했습니다. 실제 책에는 많은 사례와 그림들이 나와있으니, 실제로 사서 보시면 입문자분께 많은 도움이 될 꺼라고 생각합니다.

---

강화학습은 환경의 모델을 몰라도 환경과의 상호작용을 통하여 최적 정책을 학습합니다.  

- **예측** : 에이전트가 환경과의 상호작용을 통해 주어진 정책에 대한 가치함수를 학습. 몬테카를로 예측과 시간차 예측이 있다. 
- **제어** : 가치함수를 토대로 정책을 끊임없이 발전시켜 나가서 최적 정책을 학습. 시간차 제어인 살사가 있다. 

<br>

## 강화학습과 정책 평가 1 : 몬테카를로 예측

### 사람의 학습 방법과 강화학습의 학습 방법

강화학습은 환경의 모델을 알 필요 없이 학습합니다. 어떻게 모델 없이 문제를 해결할까요? 사람은 바둑의 모든 상황을 다 계산해서 수를 두지 않고, 둔 후에 어디가 잘 못 되었는지 파악합니다. 

강화학습은 아래와 같은 과정을 반복합니다. 강화학습은 계산을 통해서 가치함수를 알아내는 것이 아니라 에이전트가 겪은 **경험으로부터 가치함수를 업데이트 합니다.** 

- 일단 수행한다
- 평가한다
- 평가한 대로 자신을 업데이트 한다. 

### 강화학습의 예측과 제어 

다이내믹 프로그래밍의 정책 이터레이션은 정책 평가와 정책 발전으로 이루어져 있습니다. 평책 평가는 현재 정책에 대하여 가치함수를 계산합니다. 

사람은 어떤 것을 판단할 때 항상 정확한 정보를 근거로 판단하는 것이 아니라 적당한 추론을 통하여 학습합니다. **강화학습도 추론을 이용합니다.** 추론을 통하여 참 가치함수의 값을 예측합니다. 

### 몬테카를로 근사의 예시

원의 넓이를 계산하기 위해서는 원의 방정식을 알아야 합니다. 하지만 `몬테카를로 근사`를 이용하면 공식이 필요 없습니다. 근사는 샘플을 통하여 원래 값을 유추하는 것입니다. 수가 많아지면 원래의 값과 비슷해집니다.

![image](/assets/images/20190512/montecarro_prediction_img.jpg){:.rounded.img_center.width_40}
**몬테카를로 근사를 이용하여 넓이를 구하는 예시**{:.caption}

$$ 
\frac{\text{원의 넓이}}{\text{사각형의 넓이}}
$$

**수식.** 원의 넓이의 삼각형의 넓이에 대한 비
{:.mathjax_caption}

무한이 많이 점을 찍으면, 위의 식은 실제 값과 비슷하게 도달하게 됩니다. 

### 샘플링과 몬테카를로 예측

위의 사례에서 점 하나하나는 `샘플`입니다. 점을 찍는 것은 `샘플링`이라고 합니다. 몬테카를로 근사를 사용해 측하는 것을 `몬테카를로 예측`이라고 합니다. 

에이전트가 현재의 정책을 토대로 한 에피소드를 진행하면 반환값은 에피소드 동안 지나쳐 왔던 각각의 상태마다 존재합니다. 하나의 에피소드를 진행하는 것은 위의 그림에서 점을 여러개 찍어 보는 것과 같습니다. 

$$
  \text{v}_{\pi}(s) = \sum_{a \in A} \pi (a | s) (\text{R}_{t+1} + \gamma \sum_{s' \in S} \text{P}^a_{ss'} \text{v}_{\pi} (s') )
$$

**수식.** 계산 가능한 벨만 기대 방정식
{:.mathjax_caption}

계산 가능한 벨만 기대 방정식은 위와 같습니다. 하지만 위의 식을 계산하려면 환경 모델인 $ \text{P}^a_{ss'} $와 $ \text{R}_{t+1} $ 를 알아야 합니다. 

몬테카를로 예측에서는 환경의 모델을 몰라도 여러 에피소드를 통해 구한 반환값의 평균을 통해서 $ \text{v}_{\pi} (s) $ 를 추정합니다. 

$$
  \text{v}_{\pi} (\text{s}) \sim \frac{1}{\text{N(s)}} \sum^{\text{N(s)}}_{\text{i} = 1} \text{G}_\text{i} (\text{s})
$$

**수식.** 반환값의 평균으로 가치함수 측정하기
{:.mathjax_caption}

$$ 
  \text{V}_\text{n}= \frac{1}{\text{n-1}} \sum_{\text{i = 1}}^{\text{n-1}} \text{G}_\text{i}
$$

**수식.** 이전의 가치함수 $ \text{V}_\text{n}$
{:.mathjax_caption}

이전 가치함수는 위와 같고, 새로 갱신한 $ \text{V}_\text{n+1} $를 갱신하면 아래와 같이 나타낼 수 있습니다. 

$$
  \text{V}_\text{n+1} \leftarrow \text{V}_{n} + \frac{1}{\text{n}} (\text{G} (s) - \text{V} (s) ) 
$$

$$
  \text{V}_\text{n+1} \leftarrow \text{V}_{n} + \alpha (\text{G} (s) - \text{V} (s) ) 
$$

**수식.** 가치 함수 갱신
{:.mathjax_caption}

가치함수에서 $ \text{G} (s) - \text{V} (s) $ 는 오차라고 하며, $ \frac{1}{n} $ 은 스텝사이즈로서 업데이트 할 때, 얼마만큼의 오차를 가지고 업데이트 할 지 정하는 것입니다. 스텝사이즈는 일반적으로 $ \alpha $ 라고 표현합니다. 

<br>

## 강화학습과 정책 평가 2 : 시간차 예측

### 시간차 예측

몬테카를로 예측은 실시간이 아닙니다. 에피소드가 기다려야 할 때까지 기다려야 되는데, 에피소드가 길거나 끝이 없으면 적합하지 않습니다.

에피소드마다가 아니라 타임스텝마다 가치함수를 업데이트 하는 방법이 `시간차 예측`입니다. 

$$
  \text{v}_{\pi} (s) = \text{E}_{\pi} [\text{G}_\text{t} | \text{S}_\text{t} = \text{s}]
$$

**수식.** 몬테카를로 예측은 샘플링으로 값을 예측. 즉, 반환값의 기대값으로 가치함수 정의
{:.mathjax_caption}

몬테카를로 예측에서는 위와 같이 반환값의 기대값을 이용하여 가치함수를 정의하였는데 시간차 예측에서는 $ \text{G}_\text{t} $를 $ \text{R}_{\text{t+1}} + \gamma \text{v}_\pi(s') $ 으로 나타낸 아래의 식을 나타냅니다. 

$$
  \text{v}_{\pi} (s) = \text{E}_{\pi} [\text{R}_{\text{t+1}} + 
  \gamma \text{v}_\pi ( \text{S} _ {\text{t+1}} ) | \text{S}_\text{t} = \text{s}]
$$

**수식.** 시간차 예측에서의 가치함수 정의
{:.mathjax_caption}

매 타임스텝마다 에이전트는 현재의 상태 $ \text{S}_{\text{t}} $ 에서 행동을 하나 선택하고 환경으로 보상을 받고 다음 상태 $ \text{S}_{\text{t+1}} $ 를 알게됩니다. 

$$
  \text{V} (\text{S}_\text{t}) \leftarrow \text{V} (\text{S}_\text{t}) + \alpha (\text{R} + \gamma \text{V} (\text{S}_{\text{t+1}} ) - \text{V} (\text{S}_\text{t}) ) 
$$

**수식.** 시간차 예측에서의 가치함수 업데이트
{:.mathjax_caption}

$ \text{R} + \gamma \text{V} (\text{S}_{\text{t+1}} ) - \text{V} (\text{S}_\text{t}) $ 는 시간차 에러라고 합니다. $ \text{V} (\text{S}_{\text{t+1}} ) $ 는 에이전트 현재 가지고 있는 값입니다. 에이전트는 이 값을 $ \text{S}_\text{t} $ 라고 예측하고 있습니다. 다른 상태의 가치함수 예측값을 통해, 지금 상태의 가치함수를 예측하는 방식을 `부트스트랩`이라고 합니다. **업데이트 목표가 정확하지 않은 상태에서 가치 함수를 업데이트 하는 것입니다.**

![image](/assets/images/20190512/time_prediction_value_update.jpg){:.rounded.img_center.width_40}
**시간차 예측의 가치함수 업데이트**{:.caption}

<br>

## 강화학습 알고리즘 1 : 살사

### 살사

<br>

## 강화학습 알고리즘 2 : 큐러닝

### 살사의 한계

### 큐러닝 이론

