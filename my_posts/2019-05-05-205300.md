---
layout : article
title: 강화학습의 가치 함수와 벨만 방정식의 개념
aside:
  toc: true
tags: MachineLearning
category : MachineLearning
author: melonicedlatte
published : True
# cover : /assets/images/logo/chihuahua.jpg
key : 2019-05-05-205300
mathjax: true 
---

*파이썬과 케라스로 배우는 강화학습* 책을 보고 간략하게 배운 개념을 요약했습니다. 실제 책에는 많은 사례와 그림들이 나와있으니, 실제로 사서 보시면 입문자분께 많은 도움이 될 꺼라고 생각합니다.

<br>

## 가치함수

에이전트가 최적 정책을 찾기 위해서는 **앞으로 받을 보상들을 고려**해야 합니다. 아직 받지 않은 보상들을 고려하기 위하여 가치함수가 필요합니다.

$$ R_{t+1} + R_{t+2} + R_{t+3} + \cdots$$

**수식.** 보상들의 합
{:.mathjax_caption}

위와 같이 **감가하지 않고 보상들을 더하면 아래와 같은 문제**가 생길 수 있습니다.

1. 지금 받은 보상이나 미래에 받는 보상이 같아 구분할 수 없습니다.
2. 큰 보상을 한 번 받은 것과 작은 보상을 여러번 받은 것을 구분할 수 없습니다.
3. 시간이 무한대이면 10을 계속 더한 것과 100을 계속 더한 것을 구분할 수 없습니다.

감가율을 고려하면 위의 식을 아래와 같이 나타낼 수 있고, 이를 반환값 $G_t$ 라고 합니다.

$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3} + \cdots$$

**수식.** 보상들의 합
{:.mathjax_caption}

에이전트는 위와 같은 반환값 $G_t$를 알기 위해서 에피소드가 모두 끝날 때 까지 기다려야 합니다. 하지만, 때로는 정확한 값을 얻기 위해 끝까지 기다리는 것보다 **정확하지 않더라도 현재의 정보를 가지고 행동**하는 것이 나을 때가 있습니다.

어떤 상태에 있으면 앞으로 얼마의 보상을 받을 것인지에 대한 기댓값을 고려해볼 수 있는데 이를 **가치함수**라고 합니다.

$$ \text{v}(s) = E[G_t|S_t = s] $$

**수식.** 가치함수의 정의
{:.mathjax_caption}

반환값 $G_t$는 보상이 모두 확률변수이기 때문에 확률변수이지만, 가치함수는 E로 평균을 낸 기대값이기 때문에 **소문자**로 표현합니다. 가치함수는 아래와 같이 반환값으로 식을 표현할 수 있습니다.

$$ \text{v}(s) = E[G_t|S_t = s] = E[R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3}+\cdots|S_t = s]  $$

$$ \text{v}(s) = E[R_{t+1} + \gamma(R_{t+2} + \gamma^{1}R_{t+3}+\cdots)|S_t = s]  $$

$$ \text{v}(s) = E[R_{t+1} + \gamma G_{t+1}|S_t = s]  $$

**수식.** 가치함수에서 반환값 $G_t$와 $G_{t+1}$로 식 표현
{:.mathjax_caption}

반환값을 다시 가치함수로 변경하면 아래와 같이 나타낼 수 있습니다.

$$ \text{v}(s) = E[R_{t+1} + \gamma \text{v}(S_{t+1})|S_t = s]  $$

**수식.** 가치함수로 가치함수를 표현
{:.mathjax_caption}

위의 식은 정책을 고려하지 않았습니다. **어떤 상태에서 어떤 행동을 해야 할 지에 대한 정보** 인 정책을 고려하여 식을 다시 표현하면 아래와 같습니다.

$$ \text{v}_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma \text{v}_{\pi}(S_{t+1})|S_t = s]  $$

**수식.** 정책을 고려한 가치함수의 표현
{:.mathjax_caption}

### 큐함수

가치함수는 상태가 입력으로 들어오면 앞으로 받을 보상의 합을 출력하고 이는 어떤 상태에 있는 것이 더 좋은지 알 수 있게 합니다.

어떤 상태에서 할 수 있는 모든 행동에 대하여 가치함수를 확인할 수 있으면, 어떤 행동이 더 좋은지 알 수 있습니다. 주어진 상태에서 수많은 행동 중 어떤 행동이 얼마나 좋은지 알려주는 함수를 `행동 가치함수 == 큐 함수(Q Function)` 이라고 합니다.

$$ \text{v}_{\pi}(s) = \sum_{a \in A} \pi (a|s) q_{\pi}(s,a)  $$

**수식.** 큐함수의 합은 가치함수이다
{:.mathjax_caption}

$q_{\pi}(s,a)$는 상태 $s$에서 행동 $a$를 취했을 때, 얻을 수 있는 가치를 표현합니다. 따라서, $\sum$을 이용하여 모든 행동과 정책(행동을 수행할 확률)을 곱해서 얻을 수 있는 값을 모두 더하여 상태 s에 있으면 얻을 수 있는 가치를 계산합니다.

가치함수 식을 큐함수를 적용하여 바꾸면 아래와 같습니다.

$$ q_{\pi}(s,a) = E_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1})|S_t=s,A_t=a]$$

**수식.** 큐함수 정의
{:.mathjax_caption}

<br>

## 벨만 방정식



### 벨만 기대 방정식

### 벨만 최적 방정식